# üõ†Ô∏è –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ Arena —Å–∏—Å—Ç–µ–º—ã

–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏–∑–º–µ–Ω–µ–Ω–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–µ–π—Å—Ç–≤–∏–π, –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è.

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞-—Å–∏—Å—Ç–µ–º—ã)
2. [–ò–∑–º–µ–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π (Actions)](#–∏–∑–º–µ–Ω–µ–Ω–∏–µ-–¥–µ–π—Å—Ç–≤–∏–π-actions)
3. [–ò–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π (Observations)](#–∏–∑–º–µ–Ω–µ–Ω–∏–µ-–Ω–∞–±–ª—é–¥–µ–Ω–∏–π-observations)
4. [–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π](#–º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è-–º–æ–¥–µ–ª–µ–π)
5. [–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è](#–ø—Ä–æ—Ü–µ—Å—Å-–æ–±—É—á–µ–Ω–∏—è)
6. [–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã](#–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ-—Å–∏—Å—Ç–µ–º—ã)

---

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã:

```
Arena System
‚îú‚îÄ‚îÄ üèüÔ∏è Environment (arena_env.py)
‚îÇ   ‚îú‚îÄ‚îÄ ArenaEnv - –æ—Å–Ω–æ–≤–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
‚îÇ   ‚îî‚îÄ‚îÄ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Ç–∏–ø–æ–≤
‚îú‚îÄ‚îÄ üß† Models
‚îÇ   ‚îú‚îÄ‚îÄ entity_attention_model.py - –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
‚îÇ   ‚îú‚îÄ‚îÄ hierarchical_command_policy.py - –∫–æ–º–∞–Ω–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
‚îÇ   ‚îî‚îÄ‚îÄ masked_multihead_dist.py - –¥–∏—Å—Ç—Ä–∏–±—É—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π
‚îú‚îÄ‚îÄ üéØ Training (train_rllib_league.py)
‚îÇ   ‚îú‚îÄ‚îÄ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ League —Å–∏—Å—Ç–µ–º–∞
‚îÇ   ‚îî‚îÄ‚îÄ Callbacks –∏ ONNX —ç–∫—Å–ø–æ—Ä—Ç
‚îî‚îÄ‚îÄ üîÆ Inference (universal_inference.py)
    ‚îú‚îÄ‚îÄ ONNX –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
    ‚îú‚îÄ‚îÄ Ray –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
    ‚îî‚îÄ‚îÄ PyTorch –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
```

### –°–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏:

```
Environment ‚Üí Models ‚Üí Training ‚Üí Export ‚Üí Inference
     ‚Üì           ‚Üì         ‚Üì         ‚Üì        ‚Üì
   obs_space   model    algorithm  onnx   prediction
  action_space arch     config     files   results
```

---

## üéØ –ò–∑–º–µ–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π (Actions)

### –¢–µ–∫—É—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π:

```python
# –í arena_env.py –∏ masked_multihead_dist.py
action = {
    "target": Discrete(max_enemies),    # ID –≤—Ä–∞–≥–∞ (0-5)
    "move": Box(-1, 1, (2,)),          # –î–≤–∏–∂–µ–Ω–∏–µ [x, y]
    "aim": Box(-1, 1, (2,)),           # –ü—Ä–∏—Ü–µ–ª [x, y]  
    "fire": Discrete(2)                 # –°—Ç—Ä–µ–ª—è—Ç—å (0/1)
}
```

### üîß –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏—è:

#### 1. **–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "shield")**

**–§–∞–π–ª: `arena_env.py`**
```python
# –í ArenaEnv.__init__
self.single_act_space = spaces.Dict({
    "target": spaces.Discrete(self.max_enemies),
    "move": _box(-1, 1, (CONT_ACTION_MOVE,)),
    "aim": _box(-1, 1, (CONT_ACTION_AIM,)),
    "fire": spaces.Discrete(2),
    "shield": spaces.Discrete(2),  # ‚Üê –ù–û–í–û–ï –î–ï–ô–°–¢–í–ò–ï
})

# –í ArenaEnv.step() –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
shield = int(act["shield"]) if np.isscalar(act["shield"]) else int(act["shield"].item())
# –õ–æ–≥–∏–∫–∞ —â–∏—Ç–∞...
```

**–§–∞–π–ª: `masked_multihead_dist.py`**
```python
class MaskedTargetMoveAimFireShield(TorchDistributionWrapper):  # –ù–æ–≤–æ–µ –∏–º—è!
    def __init__(self, inputs, model):
        super().__init__(inputs, model)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è shield
        idx = 0
        logits_t = inputs[..., idx:idx+self.ne]; idx += self.ne
        mu_move  = inputs[..., idx:idx+2];        idx += 2
        logstd_mv= inputs[..., idx:idx+2];        idx += 2
        mu_aim   = inputs[..., idx:idx+2];        idx += 2
        logstd_am= inputs[..., idx:idx+2];        idx += 2
        logit_fr = inputs[..., idx:idx+1];        idx += 1
        logit_sh = inputs[..., idx:idx+1];        idx += 1  # ‚Üê –ù–û–í–û–ï

        self.cat = Categorical(logits=logits_t)
        self.mv  = Normal(mu_move, logstd_mv.exp())
        self.am  = Normal(mu_aim,  logstd_am.exp())
        self.fr  = Bernoulli(logits=logit_fr)
        self.sh  = Bernoulli(logits=logit_sh)  # ‚Üê –ù–û–í–û–ï
    
    @staticmethod
    def required_model_output_shape(action_space, model_config):
        max_enemies = model_config.get("custom_model_config", {}).get("max_enemies", 6)
        return max_enemies + 2 + 2 + 2 + 2 + 1 + 1  # +1 –¥–ª—è shield
    
    def sample(self):
        t = self.cat.sample().unsqueeze(-1).float()
        mv = torch.tanh(self.mv.rsample())
        am = torch.tanh(self.am.rsample())
        fr = self.fr.sample().float()
        sh = self.sh.sample().float()  # ‚Üê –ù–û–í–û–ï
        
        flat_action = torch.cat([t, mv, am, fr, sh], dim=-1)  # –î–æ–±–∞–≤–∏–ª–∏ sh
        return self._convert_to_numpy_dict(flat_action)
    
    def _convert_to_numpy_dict(self, tensor_action):
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        target = tensor_action[..., 0].long()
        move = tensor_action[..., 1:3]
        aim = tensor_action[..., 3:5]
        fire = tensor_action[..., 5].long()
        shield = tensor_action[..., 6].long()  # ‚Üê –ù–û–í–û–ï
        
        result = {
            "target": target.numpy(),
            "move": move.numpy(),
            "aim": aim.numpy(),
            "fire": fire.numpy(),
            "shield": shield.numpy(),  # ‚Üê –ù–û–í–û–ï
        }
        return result

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –Ω–æ–≤—É—é –¥–∏—Å—Ç—Ä–∏–±—É—Ü–∏—é
ModelCatalog.register_custom_action_dist("masked_multihead_shield", MaskedTargetMoveAimFireShield)
```

**–§–∞–π–ª: `entity_attention_model.py`**
```python
class EntityAttentionModel(TorchModelV2, nn.Module):
    def __init__(self, ...):
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ–ª–æ–≤—É –¥–ª—è —â–∏—Ç–∞
        self.head_shield_logit = MLP([d_model, hidden, 1])
    
    def forward(self, input_dict, state, seq_lens):
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
        
        # –î–æ–±–∞–≤–ª—è–µ–º shield –∫ –≤—ã—Ö–æ–¥—É
        logit_shield = self.head_shield_logit(h)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—ã—Ö–æ–¥
        out = torch.cat([
            masked_logits, mu_move, log_std_move, 
            mu_aim, log_std_aim, logit_fire, 
            logit_shield  # ‚Üê –ù–û–í–û–ï
        ], dim=-1)
        
        return out, state
```

**–§–∞–π–ª: `train_rllib_league.py`**
```python
# –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –¥–∏—Å—Ç—Ä–∏–±—É—Ü–∏–∏
base_model_config = {
    "custom_model": "entity_attention",
    "custom_action_dist": "masked_multihead_shield",  # –ù–æ–≤–æ–µ –∏–º—è
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–µ
}
```

#### 2. **–ò–∑–º–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è**

–ù–∞–ø—Ä–∏–º–µ—Ä, –∏–∑–º–µ–Ω–∏—Ç—å –¥–≤–∏–∂–µ–Ω–∏–µ —Å 2D –Ω–∞ 3D:

**–í `arena_env.py`:**
```python
CONT_ACTION_MOVE = 3  # –ë—ã–ª–æ 2

self.single_act_space = spaces.Dict({
    "move": _box(-1, 1, (CONT_ACTION_MOVE,)),  # –¢–µ–ø–µ—Ä—å 3D
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–µ
})
```

**–í `entity_attention_model.py`:**
```python
self.head_move_mu = MLP([d_model, hidden, 3])  # –ë—ã–ª–æ 2
self.log_std_move = nn.Parameter(torch.full((3,), -0.5))  # –ë—ã–ª–æ (2,)
```

**–í `masked_multihead_dist.py`:**
```python
mu_move = inputs[..., idx:idx+3]     # –ë—ã–ª–æ idx+2
log_std_move = inputs[..., idx:idx+3] # –ë—ã–ª–æ idx+2
```

#### 3. **–ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π**

–ù–∞–ø—Ä–∏–º–µ—Ä, RTS-—Å—Ç–∏–ª—å —Å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º —é–Ω–∏—Ç–æ–≤:

```python
# –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ arena_env.py
self.single_act_space = spaces.Dict({
    "unit_action": spaces.Discrete(5),  # MOVE, ATTACK, BUILD, GATHER, IDLE
    "target_position": _box(-10, 10, (2,)),
    "target_unit": spaces.Discrete(self.max_enemies + self.max_allies),
    "build_type": spaces.Discrete(4),   # WARRIOR, ARCHER, BUILDER, NONE
    "resource_allocation": _box(0, 1, (3,)),  # ATTACK, DEFENSE, ECONOMY
})
```

---

## üëÅÔ∏è –ò–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π (Observations)

### –¢–µ–∫—É—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π:

```python
# –í arena_env.py
obs = {
    "self": Box(-10, 10, (SELF_FEATS,)),           # 12 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    "allies": Box(-10, 10, (MAX_ALLIES, 8)),       # 6x8 —Å–æ—é–∑–Ω–∏–∫–∏  
    "enemies": Box(-10, 10, (MAX_ENEMIES, 10)),    # 6x10 –≤—Ä–∞–≥–∏
    "allies_mask": MultiBinary(MAX_ALLIES),        # 6 –º–∞—Å–∫–∞ —Å–æ—é–∑–Ω–∏–∫–æ–≤
    "enemies_mask": MultiBinary(MAX_ENEMIES),      # 6 –º–∞—Å–∫–∞ –≤—Ä–∞–≥–æ–≤
    "global_state": Box(-10, 10, (GLOBAL_FEATS,)), # 64 –≥–ª–æ–±–∞–ª—å–Ω–æ–µ
    "enemy_action_mask": MultiBinary(MAX_ENEMIES)   # 6 –º–∞—Å–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π
}
```

### üîß –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:

#### 1. **–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π —Ç–∏–ø –Ω–∞–±–ª—é–¥–µ–Ω–∏—è**

–ù–∞–ø—Ä–∏–º–µ—Ä, –∫–∞—Ä—Ç—É –º–µ—Å—Ç–Ω–æ—Å—Ç–∏:

**–í `arena_env.py`:**
```python
# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MAP_SIZE = 32
TERRAIN_CHANNELS = 3  # –≤—ã—Å–æ—Ç–∞, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è, —Ä–µ—Å—É—Ä—Å—ã

# –í ArenaEnv.__init__
self.single_obs_space = spaces.Dict({
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è ...
    "terrain_map": _box(0, 1, (TERRAIN_CHANNELS, MAP_SIZE, MAP_SIZE)),
    "minimap": _box(0, 1, (4, 16, 16)),  # 4 –∫–∞–Ω–∞–ª–∞: —Å–æ—é–∑–Ω–∏–∫–∏, –≤—Ä–∞–≥–∏, –Ω–µ–π—Ç—Ä–∞–ª—ã, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è
})

# –í _build_obs
def _build_obs(self, aid: str) -> Dict[str, np.ndarray]:
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
    
    # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É –º–µ—Å—Ç–Ω–æ—Å—Ç–∏
    terrain_map = np.zeros((TERRAIN_CHANNELS, MAP_SIZE, MAP_SIZE), dtype=np.float32)
    terrain_map[0] = self._generate_height_map()
    terrain_map[1] = self._generate_obstacles()
    terrain_map[2] = self._generate_resources()
    
    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–∫–∞—Ä—Ç—É
    minimap = self._create_minimap(aid)
    
    return {
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è ...
        "terrain_map": terrain_map,
        "minimap": minimap,
    }
```

**–í `entity_attention_model.py`:**
```python
class EntityAttentionModel(TorchModelV2, nn.Module):
    def __init__(self, ...):
        # –î–æ–±–∞–≤–ª—è–µ–º CNN –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ä—Ç
        self.terrain_cnn = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((4, 4)),
            nn.Flatten(),
            nn.Linear(64 * 16, d_model // 4)
        )
        
        self.minimap_cnn = nn.Sequential(
            nn.Conv2d(4, 16, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((4, 4)),
            nn.Flatten(),
            nn.Linear(16 * 16, d_model // 4)
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä–∞
        integrated_size = d_model//2 + d_model//4 + d_model//4 + d_model//4 + d_model//4
    
    def forward(self, input_dict, state, seq_lens):
        obs = input_dict["obs"]
        
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ ...
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ä—Ç—ã
        terrain_features = self.terrain_cnn(obs["terrain_map"])
        minimap_features = self.minimap_cnn(obs["minimap"])
        
        # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        integrated = torch.cat([
            global_encoded, allies_aggregated, enemies_aggregated,
            terrain_features, minimap_features
        ], dim=-1)
        
        # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ ...
```

#### 2. **–ò–∑–º–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π**

–ù–∞–ø—Ä–∏–º–µ—Ä, —É–≤–µ–ª–∏—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—é–∑–Ω–∏–∫–∞—Ö:

**–í `arena_env.py`:**
```python
ALLY_FEATS = 12  # –ë—ã–ª–æ 8
ENEMY_FEATS = 15  # –ë—ã–ª–æ 10

# –í _build_obs –æ–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ
def _build_obs(self, aid: str) -> Dict[str, np.ndarray]:
    # –î–ª—è —Å–æ—é–∑–Ω–∏–∫–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    allies_arr[i, :2] = self._pos[al] - self._pos[aid]  # –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è
    allies_arr[i, 2] = self._hp[al] / 100.0             # HP
    allies_arr[i, 3] = self._get_energy(al) / 100.0     # —ç–Ω–µ—Ä–≥–∏—è
    allies_arr[i, 4] = self._get_ammo(al) / 50.0        # –ø–∞—Ç—Ä–æ–Ω—ã
    allies_arr[i, 5:7] = self._get_velocity(al)         # —Å–∫–æ—Ä–æ—Å—Ç—å
    allies_arr[i, 7:9] = self._get_aim_direction(al)    # –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏—Ü–µ–ª–∞
    allies_arr[i, 9] = self._get_reload_time(al)        # –≤—Ä–µ–º—è –ø–µ—Ä–µ–∑–∞—Ä—è–¥–∫–∏
    allies_arr[i, 10] = self._get_shield_strength(al)   # –ø—Ä–æ—á–Ω–æ—Å—Ç—å —â–∏—Ç–∞
    allies_arr[i, 11] = self._get_experience(al)        # –æ–ø—ã—Ç
```

#### 3. **–ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π**

RTS-—Å—Ç–∏–ª—å —Å —ç–∫–æ–Ω–æ–º–∏–∫–æ–π:

```python
# –í arena_env.py
self.single_obs_space = spaces.Dict({
    "player_resources": _box(0, 1000, (4,)),           # –∑–æ–ª–æ—Ç–æ, –¥–µ—Ä–µ–≤–æ, –∫–∞–º–µ–Ω—å, –µ–¥–∞
    "population": _box(0, 200, (2,)),                  # —Ç–µ–∫—É—â–µ–µ, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ
    "buildings": _box(0, 1, (10, 6)),                  # 10 –∑–¥–∞–Ω–∏–π x 6 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    "buildings_mask": spaces.MultiBinary(10),
    "units": _box(-10, 10, (50, 8)),                   # 50 —é–Ω–∏—Ç–æ–≤ x 8 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    "units_mask": spaces.MultiBinary(50),
    "research_progress": _box(0, 1, (20,)),            # –ø—Ä–æ–≥—Ä–µ—Å—Å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
    "map_control": _box(0, 1, (8, 8)),                 # –∫–æ–Ω—Ç—Ä–æ–ª—å —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏
})
```

---

## üß† –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

### –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏:

#### 1. **–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ —Å–ª–æ–∏**

```python
# –í entity_attention_model.py
class EntityAttentionModel(TorchModelV2, nn.Module):
    def __init__(self, ...):
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–∏
        self.tactical_analyzer = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.ReLU(),
            nn.Linear(d_model * 2, d_model),
            nn.Dropout(0.1)
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –º–µ–∂–¥—É –∫–æ–º–∞–Ω–¥–∞–º–∏
        self.team_attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=8,
            batch_first=True
        )
    
    def forward(self, input_dict, state, seq_lens):
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
        
        # –¢–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        tactical_features = self.tactical_analyzer(h)
        
        # –í–Ω–∏–º–∞–Ω–∏–µ –º–µ–∂–¥—É –∫–æ–º–∞–Ω–¥–∞–º–∏
        team_features, _ = self.team_attention(
            tactical_features.unsqueeze(1),
            tactical_features.unsqueeze(1), 
            tactical_features.unsqueeze(1)
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        h = h + tactical_features + team_features.squeeze(1)
        
        # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ ...
```

#### 2. **–ò–∑–º–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä—ã –º–æ–¥–µ–ª–∏**

```python
# –í train_rllib_league.py
base_model_config = {
    "custom_model": "entity_attention",
    "custom_model_config": {
        "d_model": 256,        # –ë—ã–ª–æ 128
        "nhead": 16,           # –ë—ã–ª–æ 8  
        "layers": 4,           # –ë—ã–ª–æ 2
        "ff": 1024,            # –ë—ã–ª–æ 256
        "hidden": 512,         # –ë—ã–ª–æ 256
        # ... –æ—Å—Ç–∞–ª—å–Ω–æ–µ
    }
}
```

#### 3. **–ù–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏**

–ù–∞–ø—Ä–∏–º–µ—Ä, CNN + RNN –º–æ–¥–µ–ª—å:

```python
# –ù–æ–≤—ã–π —Ñ–∞–π–ª: cnn_rnn_model.py
class CNNRNNModel(TorchModelV2, nn.Module):
    def __init__(self, obs_space, action_space, num_outputs, model_config, name):
        super().__init__(obs_space, action_space, num_outputs, model_config, name)
        
        cfg = model_config.get("custom_model_config", {})
        
        # CNN –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.spatial_cnn = nn.Sequential(
            nn.Conv2d(4, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((8, 8)),
            nn.Flatten()
        )
        
        # RNN –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.temporal_rnn = nn.LSTM(
            input_size=64 * 64 + 32,  # CNN features + vector features
            hidden_size=256,
            num_layers=2,
            batch_first=True
        )
        
        # –í—ã—Ö–æ–¥–Ω—ã–µ –≥–æ–ª–æ–≤—ã
        self.action_heads = nn.ModuleDict({
            "target": nn.Linear(256, 6),
            "move": nn.Linear(256, 4),  # mu_x, std_x, mu_y, std_y
            "fire": nn.Linear(256, 1)
        })
        
        self.value_head = nn.Linear(256, 1)
    
    def forward(self, input_dict, state, seq_lens):
        obs = input_dict["obs"]
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ CNN
        spatial_features = self.spatial_cnn(obs["minimap"])
        
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        vector_features = torch.cat([
            obs["self"], 
            obs["allies"].flatten(1),
            obs["enemies"].flatten(1)
        ], dim=1)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è RNN
        rnn_input = torch.cat([spatial_features, vector_features], dim=1)
        rnn_input = rnn_input.unsqueeze(1)  # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ
        
        # RNN –æ–±—Ä–∞–±–æ—Ç–∫–∞
        rnn_out, new_state = self.temporal_rnn(rnn_input, state)
        features = rnn_out.squeeze(1)
        
        # –í—ã—Ö–æ–¥—ã
        target_logits = self.action_heads["target"](features)
        move_params = self.action_heads["move"](features)
        fire_logits = self.action_heads["fire"](features)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫–∏
        mask = obs["enemy_action_mask"].float()
        target_logits = target_logits + (1.0 - mask) * (-1e9)
        
        output = torch.cat([target_logits, move_params, fire_logits], dim=-1)
        
        self._value_out = self.value_head(features).squeeze(-1)
        
        return output, new_state
    
    def get_initial_state(self):
        # –ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è LSTM
        return [
            torch.zeros(2, 1, 256),  # hidden states
            torch.zeros(2, 1, 256)   # cell states
        ]

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
ModelCatalog.register_custom_model("cnn_rnn", CNNRNNModel)
```

---

## üéì –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è

### –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è:

```
1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
   ‚Üì
2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
   ‚Üì  
3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
   ‚Üì
4. –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Å League
   ‚Üì
5. –≠–∫—Å–ø–æ—Ä—Ç ONNX –º–æ–¥–µ–ª–µ–π
   ‚Üì
6. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
```

### üöÄ –ü–æ—à–∞–≥–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:

#### 1. **–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (5 –º–∏–Ω—É—Ç)**

```bash
# –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ –∫–æ–¥ –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install torch ray[rllib] onnxruntime gymnasium numpy

# –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã
python updated_training_script.py --test

# –ï—Å–ª–∏ —Ç–µ—Å—Ç –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ, –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ
python updated_training_script.py --iterations 100 --algo gspo
```

#### 2. **–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**

```bash
# –û–±—ã—á–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Å ONNX —ç–∫—Å–ø–æ—Ä—Ç–æ–º
python updated_training_script.py \
    --iterations 1000 \
    --algo gspo \
    --export-onnx \
    --record-battles \
    --export-every 50

# –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
python updated_training_script.py \
    --hierarchical \
    --iterations 500 \
    --algo ppo
```

#### 3. **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞**

```python
# –°–æ–∑–¥–∞–π—Ç–µ custom_config.py
from updated_training_script import *

# –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
args = argparse.Namespace(
    iterations=2000,
    algo="grpo",
    hierarchical=True,
    max_allies=8,           # –ë–æ–ª—å—à–µ —Å–æ—é–∑–Ω–∏–∫–æ–≤
    max_enemies=8,          # –ë–æ–ª—å—à–µ –≤—Ä–∞–≥–æ–≤  
    episode_len=256,        # –î–ª–∏–Ω–Ω–µ–µ —ç–ø–∏–∑–æ–¥—ã
    export_onnx=True,
    export_every=25,
    record_battles=True,
    num_workers=8,          # –ë–æ–ª—å—à–µ –≤–æ—Ä–∫–µ—Ä–æ–≤
    train_batch_size=32768, # –ë–æ–ª—å—à–∏–π –±–∞—Ç—á
)

# –ó–∞–ø—É—Å–∫ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
main_with_args(args)
```

#### 4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è**

```bash
# –í –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∑–∞–ø—É—Å—Ç–∏—Ç–µ TensorBoard
tensorboard --logdir ./logs

# –û—Ç–∫—Ä–æ–π—Ç–µ http://localhost:6006 –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫:
# - Episode reward mean  
# - League TrueSkill —Ä–µ–π—Ç–∏–Ω–≥–∏
# - Invalid actions —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
# - Curriculum –ø—Ä–æ–≥—Ä–µ—Å—Å
```

#### 5. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**

```bash
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ —Ç–µ—Å—Ç –º–æ–¥–µ–ª–µ–π
python quick_inference.py interactive

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
python quick_inference.py compare checkpoints/checkpoint_*

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –±–æ–µ–≤
# –û—Ç–∫—Ä–æ–π—Ç–µ ./battle_recordings/replay_*.html –≤ –±—Ä–∞—É–∑–µ—Ä–µ
```

### üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è:

#### –ê–ª–≥–æ—Ä–∏—Ç–º—ã:
- **PPO**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π, —Å—Ç–∞–±–∏–ª—å–Ω—ã–π
- **GSPO**: –ì—Ä—É–ø–ø–æ–≤—ã–µ advantages (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- **GRPO**: –ì—Ä—É–ø–ø–æ–≤—ã–µ returns, –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π

#### Curriculum (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É—Å–ª–æ–∂–Ω–µ–Ω–∏–µ):
```python
curriculum_schedule=[
    (0, [1], [1]),                    # –ù–∞—á–∏–Ω–∞–µ–º —Å 1v1
    (2_000_000, [1, 2], [1, 2]),      # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ 1v1, 2v2
    (8_000_000, [1, 2, 3], [1, 2, 3]), # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥
]
```

#### League (–æ–ø–ø–æ–Ω–µ–Ω—Ç—ã):
```python
opponents=6,              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–ø–æ–Ω–µ–Ω—Ç–æ–≤
eval_episodes=4,          # –≠–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
clone_every=15,           # –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–µ N –∏—Ç–µ—Ä–∞—Ü–∏–π
```

---

## üéÆ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:

```
your_project/
‚îú‚îÄ‚îÄ üìÅ checkpoints/           # Ray —á–µ–∫–ø–æ–∏–Ω—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_000100/
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint_000500/
‚îú‚îÄ‚îÄ üìÅ onnx_exports/          # ONNX –º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ latest/               # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ policy_main.onnx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ policy_main_meta.json
‚îÇ   ‚îî‚îÄ‚îÄ iter_000500/
‚îú‚îÄ‚îÄ üìÅ battle_recordings/     # –ó–∞–ø–∏—Å–∏ –±–æ–µ–≤
‚îÇ   ‚îú‚îÄ‚îÄ battle_0001.json
‚îÇ   ‚îî‚îÄ‚îÄ replay_battle_0001.html
‚îú‚îÄ‚îÄ üìÅ logs/                  # TensorBoard –ª–æ–≥–∏
‚îî‚îÄ‚îÄ üìÅ models/               # –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –º–æ–¥–µ–ª–µ–π
```

### üîÆ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ:

#### 1. **–ü—Ä–æ—Å—Ç–æ–π —Å–ª—É—á–∞–π**
```python
from universal_inference import load_latest_models, create_sample_input

# –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
manager = load_latest_models("./onnx_exports")

# –í –∏–≥—Ä–æ–≤–æ–º —Ü–∏–∫–ª–µ
def get_action_for_robot(robot):
    input_data = InferenceInput(
        self_features=robot.get_observation(),
        allies=[ally.get_observation() for ally in robot.allies],
        enemies=[enemy.get_observation() for enemy in robot.visible_enemies],
        global_state=game.get_global_state()
    )
    
    result = manager.predict("main", input_data)
    return result.action_dict
```

#### 2. **–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞**
```python
# 1. –ö–æ–º–∞–Ω–¥–∏—Ä –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–∏—Ç—É–∞—Ü–∏—é
commander_input = InferenceInput(
    self_features=np.zeros(12),  # –ö–æ–º–∞–Ω–¥–∏—Ä –Ω–µ –∏–º–µ–µ—Ç self
    allies=[robot.get_observation() for robot in team.robots],
    enemies=[enemy.get_observation() for enemy in visible_enemies],
    global_state=game.get_global_state()
)

commander_result = manager.predict("commander", commander_input)

# 2. –†–∞–∑–¥–∞–µ–º –∫–æ–º–∞–Ω–¥—ã –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è–º
for i, robot in enumerate(team.robots):
    command = create_command_for_follower(commander_result.action_dict, i)
    
    follower_input = InferenceInput(
        self_features=robot.get_observation(),
        allies=[other.get_observation() for other in team.robots if other != robot],
        enemies=[enemy.get_observation() for enemy in robot.visible_enemies],
        global_state=game.get_global_state(),
        command=command  # –ö–æ–º–∞–Ω–¥–∞ –æ—Ç –∫–æ–º–∞–Ω–¥–∏—Ä–∞
    )
    
    action = manager.predict("follower", follower_input)
    robot.execute_action(action.action_dict)
```

#### 3. **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Unity/Unreal**
```csharp
// C# wrapper –¥–ª—è Unity
using System;
using System.Diagnostics;

public class ArenaAIController : MonoBehaviour 
{
    private Process pythonProcess;
    
    void Start() 
    {
        // –ó–∞–ø—É—Å–∫–∞–µ–º Python –ø—Ä–æ—Ü–µ—Å—Å —Å –º–æ–¥–µ–ª—å—é
        pythonProcess = new Process();
        pythonProcess.StartInfo.FileName = "python";
        pythonProcess.StartInfo.Arguments = "unity_inference_server.py";
        pythonProcess.StartInfo.UseShellExecute = false;
        pythonProcess.StartInfo.RedirectStandardInput = true;
        pythonProcess.StartInfo.RedirectStandardOutput = true;
        pythonProcess.Start();
    }
    
    public RobotAction GetAction(RobotState state)
    {
        // –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        string jsonState = JsonUtility.ToJson(state);
        
        // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Python
        pythonProcess.StandardInput.WriteLine(jsonState);
        
        // –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        string jsonAction = pythonProcess.StandardOutput.ReadLine();
        
        // –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        return JsonUtility.FromJson<RobotAction>(jsonAction);
    }
}
```

### üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç–ª–∞–¥–∫–∞:

#### 1. **–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π**
```python
# –°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞: analyze_models.py
from universal_inference import *
import matplotlib.pyplot as plt

def analyze_model_performance(model_path):
    manager = UniversalInferenceManager()
    model_id = manager.load_model(model_path)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö
    scenarios = {
        "easy": create_sample_input("simple"),
        "medium": create_sample_input("complex"), 
        "hard": create_custom_scenario(allies=1, enemies=4)
    }
    
    results = {}
    for name, input_data in scenarios.items():
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        actions = []
        times = []
        
        for _ in range(100):
            start = time.time()
            result = manager.predict(model_id, input_data)
            times.append((time.time() - start) * 1000)
            actions.append(result.action_dict)
        
        results[name] = {
            "avg_time": np.mean(times),
            "std_time": np.std(times),
            "action_diversity": calculate_action_diversity(actions),
            "fire_rate": np.mean([a["fire"] for a in actions]),
            "avg_target": np.mean([a["target"] for a in actions])
        }
    
    return results

def calculate_action_diversity(actions):
    """–ò–∑–º–µ—Ä—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–µ–π—Å—Ç–≤–∏–π (—Ö–æ—Ä–æ—à–æ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏—è)"""
    targets = [a["target"] for a in actions]
    moves = [tuple(a["move"]) for a in actions]
    
    target_entropy = -sum(p * np.log(p) for p in np.bincount(targets) / len(targets) if p > 0)
    move_diversity = len(set(moves)) / len(moves)
    
    return {"target_entropy": target_entropy, "move_diversity": move_diversity}
```

#### 2. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è**
```python
# –°–æ–∑–¥–∞–Ω–∏–µ heatmap –¥–µ–π—Å—Ç–≤–∏–π
def visualize_model_behavior(model_path, output_dir="./analysis"):
    os.makedirs(output_dir, exist_ok=True)
    
    manager = UniversalInferenceManager()
    model_id = manager.load_model(model_path)
    
    # –°–æ–∑–¥–∞–µ–º grid —Å–∏—Ç—É–∞—Ü–∏–π
    positions = np.linspace(-5, 5, 20)
    heatmap = np.zeros((20, 20))
    
    for i, x in enumerate(positions):
        for j, y in enumerate(positions):
            # –í—Ä–∞–≥ –≤ –ø–æ–∑–∏—Ü–∏–∏ (x, y)
            input_data = InferenceInput(
                self_features=np.array([0, 0, 1] + [0]*9, dtype=np.float32),
                allies=[],
                enemies=[np.array([x, y, 1] + [0]*7, dtype=np.float32)],
                global_state=np.zeros(64, dtype=np.float32)
            )
            
            result = manager.predict(model_id, input_data)
            heatmap[i, j] = 1.0 if result.fire else 0.0
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º heatmap
    plt.figure(figsize=(10, 8))
    plt.imshow(heatmap, extent=[-5, 5, -5, 5], origin='lower', cmap='hot')
    plt.colorbar(label='Fire Probability')
    plt.xlabel('Enemy X Position')
    plt.ylabel('Enemy Y Position')
    plt.title(f'Fire Decision Heatmap - {os.path.basename(model_path)}')
    plt.savefig(f"{output_dir}/fire_heatmap.png", dpi=300, bbox_inches='tight')
    
    print(f"Heatmap saved to {output_dir}/fire_heatmap.png")
```

#### 3. **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π**
```python
def ab_test_models(model_a_path, model_b_path, num_battles=100):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–µ –º–æ–¥–µ–ª–∏ –≤ –ø—Ä—è–º—ã—Ö –±–æ—è—Ö"""
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    from arena_env import ArenaEnv
    env = ArenaEnv({"ally_choices": [2], "enemy_choices": [2], "episode_len": 128})
    
    manager = UniversalInferenceManager()
    model_a = manager.load_model(model_a_path, "model_a")
    model_b = manager.load_model(model_b_path, "model_b")
    
    wins_a, wins_b, draws = 0, 0, 0
    
    for battle in range(num_battles):
        obs, _ = env.reset()
        done = False
        
        while not done:
            actions = {}
            
            for agent_id, agent_obs in obs.items():
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫—É—é –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
                if agent_id.startswith("red_"):
                    model_id = "model_a"
                else:
                    model_id = "model_b"
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º obs –≤ InferenceInput
                input_data = obs_to_inference_input(agent_obs, env)
                result = manager.predict(model_id, input_data)
                actions[agent_id] = result.action_dict
            
            obs, rewards, terms, truncs, infos = env.step(actions)
            done = terms.get("__all__") or truncs.get("__all__")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
        red_total = sum(r for aid, r in rewards.items() if aid.startswith("red_"))
        blue_total = sum(r for aid, r in rewards.items() if aid.startswith("blue_"))
        
        if red_total > blue_total:
            wins_a += 1
        elif blue_total > red_total:
            wins_b += 1
        else:
            draws += 1
        
        if battle % 10 == 0:
            print(f"Battle {battle}: A={wins_a}, B={wins_b}, Draws={draws}")
    
    print(f"\nFinal Results:")
    print(f"Model A wins: {wins_a} ({wins_a/num_battles*100:.1f}%)")
    print(f"Model B wins: {wins_b} ({wins_b/num_battles*100:.1f}%)")
    print(f"Draws: {draws} ({draws/num_battles*100:.1f}%)")
    
    return {"wins_a": wins_a, "wins_b": wins_b, "draws": draws}

def obs_to_inference_input(obs, env):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç Ray observation –≤ InferenceInput"""
    return InferenceInput(
        self_features=obs["self"],
        allies=[obs["allies"][i] for i in range(len(obs["allies"])) if obs["allies_mask"][i]],
        enemies=[obs["enemies"][i] for i in range(len(obs["enemies"])) if obs["enemies_mask"][i]],
        global_state=obs["global_state"]
    )
```

---

## üîÑ –¢–∏–ø–∏—á–Ω—ã–µ workflow'—ã

### 1. **–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–π –∏–≥—Ä–æ–≤–æ–π –º–µ—Ö–∞–Ω–∏–∫–∏**

```bash
# 1. –ò–∑–º–µ–Ω—è–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
edit arena_env.py  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è/–Ω–∞–±–ª—é–¥–µ–Ω–∏—è

# 2. –û–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å
edit entity_attention_model.py  # –ù–æ–≤—ã–µ –≥–æ–ª–æ–≤—ã/—Å–ª–æ–∏

# 3. –û–±–Ω–æ–≤–ª—è–µ–º –¥–∏—Å—Ç—Ä–∏–±—É—Ü–∏—é –¥–µ–π—Å—Ç–≤–∏–π  
edit masked_multihead_dist.py  # –ù–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–µ–π—Å—Ç–≤–∏–π

# 4. –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
python updated_training_script.py --test

# 5. –ö–æ—Ä–æ—Ç–∫–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
python updated_training_script.py --iterations 50

# 6. –ü–æ–ª–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –µ—Å–ª–∏ –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
python updated_training_script.py --iterations 1000
```

### 2. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**

```bash
# 1. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
python -m cProfile -o profile.stats updated_training_script.py --iterations 10

# 2. –ê–Ω–∞–ª–∏–∑ —É–∑–∫–∏—Ö –º–µ—Å—Ç
python -c "
import pstats
p = pstats.Stats('profile.stats')
p.sort_stats('cumulative').print_stats(20)
"

# 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (—É–º–µ–Ω—å—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, ONNX —ç–∫—Å–ø–æ—Ä—Ç)
python updated_training_script.py --export-onnx --export-every 5

# 4. –ë–µ–Ω—á–º–∞—Ä–∫ ONNX vs Ray
python quick_inference.py batch

# 5. –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
```

### 3. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤**

```python
# –§–∞–π–ª: my_custom_algorithm.py
from gspo_grpo_policy import GSPOTorchPolicy
from ray.rllib.policy.sample_batch import SampleBatch

class MyCustomPolicy(GSPOTorchPolicy):
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ —Å –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–æ–π"""
    
    def postprocess_trajectory(self, sample_batch: SampleBatch, 
                             other_agent_batches=None, episode=None):
        
        # –°–Ω–∞—á–∞–ª–∞ –±–∞–∑–æ–≤—ã–π postprocessing
        sample_batch = super().postprocess_trajectory(
            sample_batch, other_agent_batches, episode)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ—é –ª–æ–≥–∏–∫—É
        rewards = sample_batch["rewards"]
        
        # –ü—Ä–∏–º–µ—Ä: –±–æ–Ω—É—Å –∑–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é
        if len(other_agent_batches) > 0:
            coordination_bonus = self.calculate_coordination_bonus(
                sample_batch, other_agent_batches)
            rewards = rewards + coordination_bonus
            sample_batch["rewards"] = rewards
        
        return sample_batch
    
    def calculate_coordination_bonus(self, sample_batch, other_batches):
        # –í–∞—à–∞ –ª–æ–≥–∏–∫–∞ –ø–æ–¥—Å—á–µ—Ç–∞ –±–æ–Ω—É—Å–∞ –∑–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é
        return np.zeros_like(sample_batch["rewards"])

# –í train_rllib_league.py –¥–æ–±–∞–≤–ª—è–µ–º:
def get_policy_class(algo_name):
    if algo_name == "my_custom":
        return MyCustomPolicy
    elif algo_name == "gspo":
        return GSPOTorchPolicy
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–µ
```

### 4. **–°–æ–∑–¥–∞–Ω–∏–µ —Ç—É—Ä–Ω–∏—Ä–∞ –º–æ–¥–µ–ª–µ–π**

```python
# tournament.py
def run_tournament(model_directory="./checkpoints"):
    import glob
    from itertools import combinations
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
    checkpoints = sorted(glob.glob(f"{model_directory}/checkpoint_*"))
    
    # –°–æ–∑–¥–∞–µ–º —Ç—É—Ä–Ω–∏—Ä–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    results = {}
    
    for model_a, model_b in combinations(checkpoints, 2):
        print(f"Battle: {os.path.basename(model_a)} vs {os.path.basename(model_b)}")
        
        result = ab_test_models(model_a, model_b, num_battles=20)
        
        results[(model_a, model_b)] = result
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏
    elo_ratings = calculate_elo_ratings(results)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open("tournament_results.json", "w") as f:
        json.dump({
            "matches": {f"{a}_vs_{b}": result for (a,b), result in results.items()},
            "elo_ratings": elo_ratings
        }, f, indent=2)
    
    print("Tournament completed! See tournament_results.json")
    
    return elo_ratings

def calculate_elo_ratings(match_results):
    """–ü—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç ELO —Ä–µ–π—Ç–∏–Ω–≥–æ–≤"""
    models = set()
    for (a, b) in match_results.keys():
        models.add(a)
        models.add(b)
    
    ratings = {model: 1000.0 for model in models}
    
    for (model_a, model_b), result in match_results.items():
        wins_a = result["wins_a"]
        wins_b = result["wins_b"]
        total = wins_a + wins_b + result["draws"]
        
        if total > 0:
            score_a = (wins_a + 0.5 * result["draws"]) / total
            
            # ELO update
            expected_a = 1 / (1 + 10 ** ((ratings[model_b] - ratings[model_a]) / 400))
            ratings[model_a] += 32 * (score_a - expected_a)
            ratings[model_b] += 32 * ((1 - score_a) - (1 - expected_a))
    
    return sorted(ratings.items(), key=lambda x: x[1], reverse=True)
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –ø–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏

### 1. **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π**
```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤ –Ω–∞—á–∞–ª–µ —Å–∫—Ä–∏–ø—Ç–∞
def check_compatibility():
    import ray
    import torch
    
    if ray.__version__.startswith("2.4"):
        print("‚úÖ Ray 2.4.x detected - compatible")
    else:
        print(f"‚ö†Ô∏è Ray {ray.__version__} - may have compatibility issues")
    
    if torch.cuda.is_available():
        print(f"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}")
    else:
        print("‚ÑπÔ∏è Using CPU training (slower)")
```

### 2. **–†–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–µ–π –∏ –ø–∞–º—è—Ç—å**
```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
def calculate_optimal_batch_size():
    available_memory = torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else 8e9
    
    # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞—Å—Ö–æ–¥ –ø–∞–º—è—Ç–∏ –Ω–∞ –æ–¥–∏–Ω sample
    memory_per_sample = 1024  # bytes
    
    optimal_batch = int(available_memory * 0.7 / memory_per_sample / 8)  # 70% –ø–∞–º—è—Ç–∏, –∑–∞–ø–∞—Å x8
    optimal_batch = max(512, min(optimal_batch, 32768))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º–∏ –ø—Ä–µ–¥–µ–ª–∞–º–∏
    
    return {
        "train_batch_size": optimal_batch,
        "minibatch_size": optimal_batch // 8
    }
```

### 3. **–û—Ç–ª–∞–¥–∫–∞ –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è**
```python
# –°–∫—Ä–∏–ø—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: debug_training.py
def diagnose_training_issues(checkpoint_dir="./checkpoints"):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ–±—É—á–µ–Ω–∏—è"""
    
    import glob
    
    checkpoints = sorted(glob.glob(f"{checkpoint_dir}/checkpoint_*"))
    
    if len(checkpoints) < 2:
        print("‚ùå Need at least 2 checkpoints for analysis")
        return
    
    print("üîç Analyzing training progression...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    performance_trend = []
    
    for i, checkpoint in enumerate(checkpoints[-5:]):  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5
        try:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            manager = UniversalInferenceManager()
            model_id = manager.load_model(checkpoint, f"model_{i}", "ray")
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
            input_data = create_sample_input("simple")
            result = manager.predict(model_id, input_data)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º "—Ä–∞–∑—É–º–Ω–æ—Å—Ç—å" –¥–µ–π—Å—Ç–≤–∏–π
            reasonableness = evaluate_action_reasonableness(result)
            performance_trend.append(reasonableness)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load {checkpoint}: {e}")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥
    if len(performance_trend) >= 3:
        if performance_trend[-1] < performance_trend[0]:
            print("üìâ Performance degrading - possible overfitting")
        elif performance_trend[-1] > performance_trend[-3]:
            print("üìà Performance improving - training healthy")
        else:
            print("üìä Performance stable - consider adjusting learning rate")
    
    return performance_trend

def evaluate_action_reasonableness(result):
    """–ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑—É–º–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π"""
    score = 0.0
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ü–µ–ª—å –≤ —Ä–∞–∑—É–º–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
    if 0 <= result.target <= 5:
        score += 1.0
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–≤–∏–∂–µ–Ω–∏–µ –Ω–µ —Å–ª–∏—à–∫–æ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ  
    move_magnitude = np.linalg.norm(result.move)
    if move_magnitude < 1.5:  # –†–∞–∑—É–º–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ
        score += 1.0
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—Ä–∏—Ü–µ–ª —Ç–æ–∂–µ —Ä–∞–∑—É–º–Ω—ã–π
    aim_magnitude = np.linalg.norm(result.aim)
    if aim_magnitude < 1.5:
        score += 1.0
    
    # –ë–æ–Ω—É—Å –∑–∞ –Ω–µ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
    if not result.fire or move_magnitude > 0.1:  # –î–≤–∏–≥–∞–µ—Ç—Å—è –ò–õ–ò –Ω–µ —Å—Ç—Ä–µ–ª—è–µ—Ç
        score += 0.5
    
    return score / 3.5  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0, 1]
```

---

## üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç **–ø–æ–ª–Ω—É—é –≥–∏–±–∫–æ—Å—Ç—å** –¥–ª—è:

- ‚úÖ **–ò–∑–º–µ–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π**: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ/—É–¥–∞–ª–µ–Ω–∏–µ/–º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –ª—é–±—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
- ‚úÖ **–ò–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö —Ç–∏–ø–æ–≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö  
- ‚úÖ **–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π**: –û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–æ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
- ‚úÖ **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏**: –õ–µ–≥–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤
- ‚úÖ **–ü—Ä–æ–¥–∞–∫—à–µ–Ω –¥–µ–ø–ª–æ—è**: ONNX —ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:

1. **–ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–≥–æ**: –ó–∞–ø—É—Å—Ç–∏—Ç–µ `--test` —Ä–µ–∂–∏–º
2. **–ü–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ**: –ò–∑–º–µ–Ω–∏—Ç–µ –æ–¥–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ/–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
3. **–û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å**: –ö–æ—Ä–æ—Ç–∫–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
4. **–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ**: –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX –∏ –±–µ–Ω—á–º–∞—Ä–∫
5. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–π—Ç–µ**: –ü–æ–ª–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∏ –¥–µ–ø–ª–æ–π

**–í—Å—è —Å–∏—Å—Ç–µ–º–∞ —Å–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥—É–ª—å–Ω–æ** - –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –æ–¥–Ω–æ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≥–æ—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–Ω—É—é —Ç–æ—á–∫—É –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ –≤–∞—à–∏ –Ω—É–∂–¥—ã! ü§ñ‚öîÔ∏è