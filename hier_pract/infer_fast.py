#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –¥–ª—è Arena –º–æ–¥–µ–ª–µ–π
–ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
"""

import os
import sys
import argparse
import json
from pathlib import Path

def find_models():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö"""
    
    search_locations = [
        "./onnx_exports/latest/*.onnx",
        "./onnx_exports/*/*.onnx",
        "./checkpoints/checkpoint_*",
        "./*.onnx",
        "./models/*.onnx",
        "./saved_models/*.pt",
    ]
    
    import glob
    found = {}
    
    for pattern in search_locations:
        files = glob.glob(pattern)
        for file in files:
            name = Path(file).stem
            if "checkpoint" in file:
                # –î–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –±–µ—Ä–µ–º –Ω–æ–º–µ—Ä
                try:
                    num = file.split("checkpoint_")[-1]
                    name = f"checkpoint_{num}"
                except:
                    name = Path(file).name
            
            found[name] = {
                "path": file,
                "type": "onnx" if file.endswith(".onnx") else 
                        "torch" if file.endswith((".pt", ".pth")) else "ray",
                "size": os.path.getsize(file)
            }
    
    return found

def quick_test(model_path: str, model_type: str = "auto"):
    """–ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    
    try:
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–º–ø–æ—Ä—Ç —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å –µ—Å–ª–∏ –Ω–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        from infer import UniversalInferenceManager, create_sample_input
        
        manager = UniversalInferenceManager()
        model_id = manager.load_model(model_path, model_type=model_type)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏
        scenarios = ["simple", "complex", "minimal"]
        
        print(f"üéØ Testing {model_id}:")
        
        for scenario in scenarios:
            try:
                input_data = create_sample_input(scenario)
                result = manager.predict(model_id, input_data)
                
                print(f"  {scenario:8}: ‚úÖ {result}")
                
            except Exception as e:
                print(f"  {scenario:8}: ‚ùå {e}")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Missing dependencies: {e}")
        print("Install with: pip install torch ray onnxruntime")
        return False
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        return False

def interactive_menu():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –º–µ–Ω—é –¥–ª—è –≤—ã–±–æ—Ä–∞ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    print("üîç Searching for models...")
    models = find_models()
    
    if not models:
        print("‚ùå No models found!")
        print("Expected locations:")
        print("  ./onnx_exports/latest/*.onnx")
        print("  ./checkpoints/checkpoint_*")
        print("  ./*.onnx")
        return
    
    print(f"\nüìã Found {len(models)} models:")
    model_list = list(models.items())
    
    for i, (name, info) in enumerate(model_list, 1):
        size_mb = info["size"] / (1024 * 1024)
        print(f"  {i:2d}. {name:25} ({info['type']:4}) {size_mb:6.1f}MB")
    
    while True:
        try:
            choice = input(f"\nSelect model (1-{len(models)}) or 'q' to quit: ").strip()
            
            if choice.lower() == 'q':
                break
            
            if choice.lower() == 'all':
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
                print("\nüîÑ Testing all models...")
                for name, info in models.items():
                    print(f"\n--- {name} ---")
                    quick_test(info["path"], info["type"])
                break
            
            try:
                idx = int(choice) - 1
                if 0 <= idx < len(model_list):
                    name, info = model_list[idx]
                    print(f"\nüß™ Testing {name}...")
                    
                    success = quick_test(info["path"], info["type"])
                    
                    if success:
                        print(f"‚úÖ {name} working correctly!")
                    else:
                        print(f"‚ùå {name} test failed")
                    
                    continue_choice = input("\nTest another? (y/n): ").strip().lower()
                    if continue_choice != 'y':
                        break
                else:
                    print(f"Invalid choice. Enter 1-{len(models)}")
            except ValueError:
                print("Invalid input. Enter a number or 'q'")
                
        except KeyboardInterrupt:
            print("\nExiting...")
            break

def batch_test(output_file: str = None):
    """–ü–∞–∫–µ—Ç–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    models = find_models()
    if not models:
        print("No models found for batch testing")
        return
    
    results = {}
    
    print(f"üîÑ Batch testing {len(models)} models...")
    
    for name, info in models.items():
        print(f"\nTesting {name}...")
        
        try:
            from universal_inference import UniversalInferenceManager, create_sample_input
            import time
            
            manager = UniversalInferenceManager()
            model_id = manager.load_model(info["path"], model_type=info["type"])
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π
            input_data = create_sample_input("simple")
            
            # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
            start_time = time.time()
            result = manager.predict(model_id, input_data)
            inference_time = (time.time() - start_time) * 1000
            
            results[name] = {
                "status": "success",
                "inference_time_ms": inference_time,
                "model_type": result.model_type,
                "result": str(result),
                "file_size_mb": info["size"] / (1024 * 1024),
                "file_type": info["type"]
            }
            
            print(f"  ‚úÖ {inference_time:.2f}ms")
            
        except Exception as e:
            results[name] = {
                "status": "failed", 
                "error": str(e),
                "file_size_mb": info["size"] / (1024 * 1024),
                "file_type": info["type"]
            }
            print(f"  ‚ùå {e}")
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    print(f"\nüìä Batch Test Results:")
    print(f"{'Model':<25} {'Status':<8} {'Time':<10} {'Type':<6} {'Size':<8}")
    print("-" * 70)
    
    for name, result in results.items():
        if result["status"] == "success":
            time_str = f"{result['inference_time_ms']:.1f}ms"
            print(f"{name:<25} {'‚úÖ':<8} {time_str:<10} {result['file_type']:<6} {result['file_size_mb']:.1f}MB")
        else:
            print(f"{name:<25} {'‚ùå':<8} {'FAILED':<10} {result['file_type']:<6} {result['file_size_mb']:.1f}MB")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if output_file:
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nüíæ Results saved to {output_file}")
    
    return results

def main():
    parser = argparse.ArgumentParser(description="Quick Arena Model Inference")
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    interactive_parser = subparsers.add_parser('interactive', help='Interactive model selection')
    
    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
    test_parser = subparsers.add_parser('test', help='Test specific model')
    test_parser.add_argument('model_path', help='Path to model')
    test_parser.add_argument('--type', choices=['onnx', 'torch', 'ray', 'auto'], 
                           default='auto', help='Model type')
    
    # –ü–∞–∫–µ—Ç–Ω—ã–π —Ç–µ—Å—Ç
    batch_parser = subparsers.add_parser('batch', help='Test all found models')
    batch_parser.add_argument('--output', help='Save results to JSON file')
    
    # –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π
    find_parser = subparsers.add_parser('find', help='Find available models')
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    compare_parser = subparsers.add_parser('compare', help='Compare models')
    compare_parser.add_argument('models', nargs='+', help='Model paths to compare')
    
    args = parser.parse_args()
    
    if args.command == 'interactive' or not args.command:
        interactive_menu()
    
    elif args.command == 'test':
        if not os.path.exists(args.model_path):
            print(f"‚ùå File not found: {args.model_path}")
            return 1
        
        success = quick_test(args.model_path, args.type)
        return 0 if success else 1
    
    elif args.command == 'batch':
        batch_test(args.output)
    
    elif args.command == 'find':
        models = find_models()
        if models:
            print(f"Found {len(models)} models:")
            for name, info in models.items():
                size_mb = info["size"] / (1024 * 1024)
                print(f"  {name:25} ({info['type']:4}) {size_mb:6.1f}MB - {info['path']}")
        else:
            print("No models found")
    
    elif args.command == 'compare':
        try:
            from infer import UniversalInferenceManager, create_sample_input
            
            manager = UniversalInferenceManager()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            for model_path in args.models:
                if os.path.exists(model_path):
                    manager.load_model(model_path)
                else:
                    print(f"‚ö†Ô∏è Not found: {model_path}")
            
            if not manager.engines:
                print("‚ùå No models loaded successfully")
                return 1
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º
            input_data = create_sample_input("simple")
            results = manager.predict_multiple(input_data)
            
            print(f"\nüìä Comparison Results:")
            for model_id, result in results.items():
                if result:
                    print(f"  {model_id}: {result}")
                else:
                    print(f"  {model_id}: ‚ùå Failed")
        
        except ImportError:
            print("‚ùå Missing dependencies for comparison")
            return 1
    
    else:
        parser.print_help()
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())s