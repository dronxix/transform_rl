"""
–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ callbacks —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º ONNX —ç–∫—Å–ø–æ—Ä—Ç–æ–º –∏ –∑–∞–ø–∏—Å—å—é –±–æ–µ–≤
–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä algorithm –≤ on_episode_end
"""

import os
import numpy as np
from typing import Dict, Any, List, Optional
import ray
from torch.utils.tensorboard import SummaryWriter

from ray.rllib.callbacks.callbacks import RLlibCallback
from ray.rllib.algorithms import Algorithm

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏
from onnx_callbacks import export_onnx_with_meta
from save_res import BattleRecorder, RecordingArenaWrapper

class FixedLeagueCallbacksWithONNXAndRecording(RLlibCallback):
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï callbacks —Å:
    1. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–º ONNX —ç–∫—Å–ø–æ—Ä—Ç–æ–º —Å meta.json —Ñ–∞–π–ª–∞–º–∏
    2. –ó–∞–ø–∏—Å—å—é –±–æ–µ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    3. –£–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    4. –ò–°–ü–†–ê–í–õ–ï–ù–ê —Å–∏–≥–Ω–∞—Ç—É—Ä–∞ on_episode_end –¥–ª—è Ray 2.48+
    """
    
    def __init__(self):
        super().__init__()
        self.league = None
        self.opponent_ids = None
        self.eval_eps = 6
        self.clone_every = 10
        self.sample_top_k = 3
        self.attn_log_every = 20
        self.writer: Optional[SummaryWriter] = None
        self.curriculum = None
        
        # ONNX —ç–∫—Å–ø–æ—Ä—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.export_onnx = True
        self.export_every = 25
        self.export_dir = "./onnx_exports"
        self.policies_to_export = ["main"]
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–ø–∏—Å–∏ –±–æ–µ–≤
        self.record_battles = True
        self.battle_recorder: Optional[BattleRecorder] = None
        self.recording_frequency = 10  # –ó–∞–ø–∏—Å—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π evaluation –º–∞—Ç—á
        self.recorded_matches = 0
        
    def setup(self, league_actor, opponent_ids: List[str], **kwargs):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ callbacks"""
        self.league = league_actor
        self.opponent_ids = opponent_ids
        self.eval_eps = kwargs.get('eval_episodes', 6)
        self.clone_every = kwargs.get('clone_every_iters', 10)
        self.sample_top_k = kwargs.get('sample_top_k', 3)
        self.attn_log_every = kwargs.get('attn_log_every', 20)
        self.curriculum = kwargs.get('curriculum_schedule', [])
        
        # ONNX –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.export_onnx = kwargs.get('export_onnx', True)
        self.export_every = kwargs.get('export_every', 25)
        self.export_dir = kwargs.get('export_dir', "./onnx_exports")
        self.policies_to_export = kwargs.get('policies_to_export', ["main"])
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–ø–∏—Å–∏ –±–æ–µ–≤
        self.record_battles = kwargs.get('record_battles', True)
        self.recording_frequency = kwargs.get('recording_frequency', 10)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if self.export_onnx:
            os.makedirs(self.export_dir, exist_ok=True)
        
        if self.record_battles:
            recordings_dir = kwargs.get('recordings_dir', "./battle_recordings")
            self.battle_recorder = BattleRecorder(recordings_dir)
            print(f"üìπ Battle recording enabled, saving to: {recordings_dir}")

    def on_algorithm_init(self, *, algorithm: Algorithm, **kwargs) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è algorithm"""
        pass

    def on_train_result(self, *, algorithm: Algorithm, result: Dict[str, Any], **kwargs) -> None:
        """–û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏"""
        if self.league is None:
            return
            
        # –°–æ–∑–¥–∞–µ–º writer
        if self.writer is None:
            logdir = getattr(algorithm, "logdir", "./logs")
            self.writer = SummaryWriter(log_dir=logdir)

        it = result["training_iteration"]
        ts_total = result.get("timesteps_total", 0)

        # 1) Evaluation –º–∞—Ç—á–µ–π —Å –≤–æ–∑–º–æ–∂–Ω–æ–π –∑–∞–ø–∏—Å—å—é
        try:
            for pid in self.opponent_ids:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å —ç—Ç–æ—Ç –º–∞—Ç—á
                should_record = (
                    self.record_battles and 
                    self.battle_recorder and 
                    self.recorded_matches % self.recording_frequency == 0
                )
                
                w_main, w_opp = self._play_match(
                    algorithm, pid, self.eval_eps, 
                    record_battle=should_record,
                    battle_id=f"eval_it{it:04d}_vs_{pid}"
                )
                
                ray.get(self.league.update_pair_result.remote(w_main, w_opp, pid))
                self.recorded_matches += 1
                
        except Exception as e:
            print(f"Error in match evaluation: {e}")

        # 2) –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
        try:
            scores = ray.get(self.league.get_all_scores.remote())
            result.setdefault("custom_metrics", {})
            
            for k, (mu, sigma) in scores.items():
                result["custom_metrics"][f"ts_{k}_mu"] = mu
                result["custom_metrics"][f"ts_{k}_sigma"] = sigma
                
                conservative_score = mu - 3 * sigma
                self.writer.add_scalar(f"ts/{k}_conservative", conservative_score, it)
                
        except Exception as e:
            print(f"Error getting league scores: {e}")
            scores = {}

        # 3) –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ö—É–¥—à–µ–≥–æ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞
        if it % self.clone_every == 0 and it > 0 and scores:
            try:
                items = [(pid, scores[pid][0] - 3*scores[pid][1]) for pid in self.opponent_ids]
                worst = min(items, key=lambda z: z[1])[0]
                
                w = algorithm.get_policy("main").get_weights()
                algorithm.get_policy(worst).set_weights(w)
                ray.get(self.league.clone_main_into.remote(worst))
                
                result["custom_metrics"][f"league_refresh_{worst}"] = it
                print(f"üîÑ Refreshed opponent {worst} at iteration {it}")
                
            except Exception as e:
                print(f"Error refreshing opponent: {e}")

        # 4) –ö—É—Ä—Ä–∏–∫—É–ª—É–º
        if self.curriculum:
            for threshold, ac, ec in reversed(self.curriculum):
                if ts_total >= threshold:
                    try:
                        self._apply_curriculum(algorithm, ac, ec)
                        result["custom_metrics"]["curriculum_ally_choices"] = str(ac)
                        result["custom_metrics"]["curriculum_enemy_choices"] = str(ec)
                    except Exception as e:
                        print(f"Error setting curriculum: {e}")
                    break

        # 5) –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô ONNX —ç–∫—Å–ø–æ—Ä—Ç
        if self.export_onnx and it % self.export_every == 0 and it > 0:
            try:
                print(f"\nüîß Starting ONNX export for iteration {it}...")
                
                successful_exports = export_onnx_with_meta(
                    algorithm=algorithm,
                    iteration=it,
                    export_dir=self.export_dir,
                    policies_to_export=self.policies_to_export
                )
                
                if successful_exports:
                    result["custom_metrics"]["onnx_export_iteration"] = it
                    result["custom_metrics"]["onnx_policies_exported"] = len(successful_exports)
                    print(f"‚úÖ ONNX export completed for iteration {it} ({len(successful_exports)} policies)")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –≤ TensorBoard
                    self.writer.add_scalar("export/onnx_success", 1, it)
                    for export in successful_exports:
                        self.writer.add_text(
                            f"export/onnx_{export['policy_id']}", 
                            f"Exported to {export['onnx_path']}", 
                            it
                        )
                else:
                    print(f"‚ö†Ô∏è ONNX export completed but no policies were successfully exported")
                    self.writer.add_scalar("export/onnx_success", 0, it)
                    
            except Exception as e:
                print(f"‚ùå ONNX export failed for iteration {it}: {e}")
                self.writer.add_scalar("export/onnx_success", 0, it)
                import traceback
                traceback.print_exc()

        # 6) –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–ø–∏—Å–µ–π –±–æ–µ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        if (self.record_battles and self.battle_recorder and 
            it % (self.export_every * 2) == 0 and it > 0):
            try:
                html_path = self.battle_recorder.export_for_web_visualizer()
                if html_path:
                    result["custom_metrics"]["battle_replay_exported"] = it
                    print(f"üé¨ Battle replay exported: {html_path}")
                    
            except Exception as e:
                print(f"Error exporting battle replay: {e}")

        if self.writer:
            self.writer.flush()

    def _play_match(self, algorithm: Algorithm, opp_id: str, episodes: int, 
                   record_battle: bool = False, battle_id: str = "") -> tuple:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–∞—Ç—á–∞ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∑–∞–ø–∏—Å–∏ –¥–ª—è Ray 2.48
        """
        try:
            from arena_env import ArenaEnv
            env_config = algorithm.config.env_config.copy() if hasattr(algorithm.config, 'env_config') else {}
            temp_env = ArenaEnv(env_config)
            
            # –û–±–µ—Ä—Ç—ã–≤–∞–µ–º –≤ –∑–∞–ø–∏—Å—ã–≤–∞—é—â–∏–π wrapper –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if record_battle and self.battle_recorder:
                temp_env = RecordingArenaWrapper(temp_env, self.battle_recorder)
                print(f"üìπ Recording battle: {battle_id}")
            
            wins_main, wins_opp = 0, 0
            
            for episode_idx in range(episodes):
                obs, _ = temp_env.reset()
                done = False
                
                while not done:
                    action_dict = {}
                    
                    for aid, ob in obs.items():
                        pol_id = "main" if aid.startswith("red_") else opp_id
                        pol = algorithm.get_policy(pol_id)
                        
                        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
                        try:
                            act, _, _ = pol.compute_single_action(ob, explore=False)
                        except Exception as e:
                            print(f"Error computing action for {aid}: {e}")
                            # Fallback –¥–µ–π—Å—Ç–≤–∏–µ
                            act = [0, 0.0, 0.0, 0.0, 0.0, 0]
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                        if isinstance(act, dict):
                            action_dict[aid] = act
                        else:
                            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä—å
                            act_array = np.array(act).flatten()
                            action_dict[aid] = {
                                "target": int(act_array[0]) if len(act_array) > 0 else 0,
                                "move": act_array[1:3].tolist() if len(act_array) > 2 else [0.0, 0.0],
                                "aim": act_array[3:5].tolist() if len(act_array) > 4 else [0.0, 0.0],
                                "fire": int(round(float(act_array[5]))) if len(act_array) > 5 else 0,
                            }
                    
                    obs, rews, terms, truncs, infos = temp_env.step(action_dict)
                    done = terms.get("__all__", False) or truncs.get("__all__", False)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
                red_sum = sum(v for k, v in rews.items() if k.startswith("red_"))
                blue_sum = sum(v for k, v in rews.items() if k.startswith("blue_"))
                
                if red_sum > blue_sum:
                    wins_main += 1
                elif blue_sum > red_sum:
                    wins_opp += 1
                    
            # –ï—Å–ª–∏ –∑–∞–ø–∏—Å—ã–≤–∞–ª–∏ –±–æ–π, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            if record_battle and hasattr(temp_env, 'recorder'):
                stats = temp_env.recorder.get_summary_statistics()
                print(f"üìä Battle {battle_id} stats: {stats}")
                    
            return wins_main, wins_opp
            
        except Exception as e:
            print(f"Error in _play_match: {e}")
            import traceback
            traceback.print_exc()
            return 0, 0

    def _apply_curriculum(self, algorithm, ally_choices, enemy_choices):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫—É—Ä—Ä–∏–∫—É–ª—É–º–∞ –¥–ª—è Ray 2.48"""
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞
            if hasattr(algorithm.config, 'env_config'):
                algorithm.config.env_config["ally_choices"] = ally_choices
                algorithm.config.env_config["enemy_choices"] = enemy_choices
                print(f"üìö Updated curriculum: allies={ally_choices}, enemies={enemy_choices}")
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –æ–∫—Ä—É–∂–µ–Ω–∏—è–º
            try:
                if hasattr(algorithm, 'env_runner_group') and algorithm.env_runner_group:
                    def set_curriculum_fn(env):
                        if hasattr(env, 'set_curriculum'):
                            env.set_curriculum(ally_choices, enemy_choices)
                        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–∞—à wrapper, –æ–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
                        elif hasattr(env, 'env') and hasattr(env.env, 'set_curriculum'):
                            env.env.set_curriculum(ally_choices, enemy_choices)
                    
                    algorithm.env_runner_group.foreach_env(set_curriculum_fn)
                    print(f"‚úÖ Applied curriculum to env_runners")
                    
            except (AttributeError, Exception) as e:
                print(f"‚ö†Ô∏è Could not apply curriculum to existing envs: {e}")
                
        except Exception as e:
            print(f"‚ùå Could not apply curriculum: {e}")

    def on_episode_end(self, *, base_env, policies: Dict[str, Any], 
                      episode, env_index: Optional[int] = None, **kwargs) -> None:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±—Ä–∞–Ω –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä algorithm –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Ray 2.48+
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è —ç–ø–∏–∑–æ–¥–∞
        """
        
        try:
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —ç–ø–∏–∑–æ–¥–∞
            if hasattr(episode, 'custom_metrics'):
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ –∫–æ–º–∞–Ω–¥–∞—Ö (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞)
                red_agents = [aid for aid in episode.get_agents() if aid.startswith("red_")]
                blue_agents = [aid for aid in episode.get_agents() if aid.startswith("blue_")]
                
                episode.custom_metrics["red_team_size"] = len(red_agents)
                episode.custom_metrics["blue_team_size"] = len(blue_agents)
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—ã–∂–∏–≤—à–∏–µ
                if hasattr(base_env, 'get_sub_environments'):
                    try:
                        sub_envs = base_env.get_sub_environments()
                        if sub_envs and env_index is not None and len(sub_envs) > env_index:
                            env = sub_envs[env_index]
                            if hasattr(env, '_alive_red') and hasattr(env, '_alive_blue'):
                                red_alive = sum(env._alive_red.values()) if env._alive_red else 0
                                blue_alive = sum(env._alive_blue.values()) if env._alive_blue else 0
                                
                                episode.custom_metrics["red_survivors"] = red_alive
                                episode.custom_metrics["blue_survivors"] = blue_alive
                                
                                total_agents = len(red_agents) + len(blue_agents)
                                if total_agents > 0:
                                    episode.custom_metrics["survival_rate"] = (red_alive + blue_alive) / total_agents
                                
                    except Exception as e:
                        # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–∫—Ä—É–∂–µ–Ω–∏—è
                        pass
        except Exception as e:
            # –ü–æ–ª–Ω–æ—Å—Ç—å—é –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ - –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            pass

    def on_sample_end(self, *, samples, **kwargs) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        try:
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É invalid actions –µ—Å–ª–∏ –µ—Å—Ç—å
            if hasattr(samples, 'data') and "infos" in samples.data:
                try:
                    infos = samples.data["infos"]
                    if len(infos) > 0 and isinstance(infos[0], dict):
                        
                        total_invalid_target = sum(info.get("invalid_target", 0) for info in infos)
                        total_oob_move = sum(info.get("oob_move", 0) for info in infos)
                        total_oob_aim = sum(info.get("oob_aim", 0) for info in infos)
                        
                        if total_invalid_target > 0 or total_oob_move > 0 or total_oob_aim > 0:
                            if self.writer:
                                it = getattr(samples, 'iteration', 0) if hasattr(samples, 'iteration') else 0
                                self.writer.add_scalar("validation/invalid_targets", total_invalid_target, it)
                                self.writer.add_scalar("validation/oob_moves", total_oob_move, it)
                                self.writer.add_scalar("validation/oob_aims", total_oob_aim, it)
                                
                            print(f"‚ö†Ô∏è Invalid actions: targets={total_invalid_target}, "
                                  f"oob_moves={total_oob_move}, oob_aims={total_oob_aim}")
                            
                except Exception as e:
                    # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    pass
        except Exception as e:
            # –ü–æ–ª–Ω–æ—Å—Ç—å—é –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            pass


def create_fixed_callbacks_factory():
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö callbacks
    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å ONNX —ç–∫—Å–ø–æ—Ä—Ç–æ–º –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø–∏—Å—å –±–æ–µ–≤
    """
    def create_callbacks():
        callbacks = FixedLeagueCallbacksWithONNXAndRecording()
        callbacks.setup(
            league_actor=None,  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ –≤ main script
            opponent_ids=[],    # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ –≤ main script
            eval_episodes=4,
            clone_every_iters=15,
            curriculum_schedule=[
                (0, [1], [1]),
                (2_000_000, [1, 2], [1, 2]),
                (8_000_000, [1, 2, 3], [1, 2, 3]),
            ],            
            # ONNX —ç–∫—Å–ø–æ—Ä—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–û)
            export_onnx=True,
            export_every=25,  # –ö–∞–∂–¥—ã–µ 25 –∏—Ç–µ—Ä–∞—Ü–∏–π
            export_dir="./onnx_exports_fixed",
            policies_to_export=["main"],
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–ø–∏—Å–∏ –±–æ–µ–≤
            record_battles=True,
            recording_frequency=5,  # –ó–∞–ø–∏—Å—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π 5-–π evaluation –º–∞—Ç—á
            recordings_dir="./battle_recordings",
        )
        return callbacks
    
    return create_callbacks


# –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–¥–∏–º –ø—Ä–æ—Å—Ç—É—é —É—Ç–∏–ª–∏—Ç—É –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ONNX —ç–∫—Å–ø–æ—Ä—Ç–∞
def test_onnx_export_standalone():
    """
    –û—Ç–¥–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ONNX —ç–∫—Å–ø–æ—Ä—Ç–∞
    –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    """
    import ray
    from ray.rllib.algorithms.ppo import PPOConfig
    from ray.tune.registry import register_env
    from arena_env import ArenaEnv
    from entity_attention_model import ONNXEntityAttentionModel
    from masked_multihead_dist import MaskedTargetMoveAimFire
    from ray.rllib.models import ModelCatalog
    
    def env_creator(cfg): 
        return ArenaEnv(cfg)
    
    print("üß™ Testing standalone ONNX export...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ray
    ray.init(ignore_reinit_error=True)
    
    try:
        register_env("ArenaEnv", env_creator)
        ModelCatalog.register_custom_model("entity_attention", ONNXEntityAttentionModel)
        ModelCatalog.register_custom_action_dist("masked_multihead", MaskedTargetMoveAimFire)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –æ–∫—Ä—É–∂–µ–Ω–∏—è
        tmp_env = ArenaEnv({"ally_choices": [1], "enemy_choices": [1]})
        obs_space = tmp_env.observation_space
        act_space = tmp_env.action_space
        max_enemies = obs_space["enemies"].shape[0]
        max_allies = obs_space["allies"].shape[0]
        
        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = (
            PPOConfig()
            .api_stack(
                enable_rl_module_and_learner=False,
                enable_env_runner_and_connector_v2=False,
            )
            .environment(
                env="ArenaEnv",
                env_config={
                    "episode_len": 50,
                    "ally_choices": [1],
                    "enemy_choices": [1],
                }
            )
            .framework("torch")
            .env_runners(num_env_runners=0)  # –¢–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–π worker
            .training(train_batch_size=512, minibatch_size=256)
            .multi_agent(
                policies={
                    "main": (None, obs_space, act_space, {
                        "model": {
                            "custom_model": "entity_attention",
                            "custom_action_dist": "masked_multihead",
                            "custom_model_config": {
                                "max_enemies": max_enemies,
                                "max_allies": max_allies,
                            },
                            "vf_share_layers": False,
                        }
                    }),
                },
                policy_mapping_fn=lambda aid, *args, **kwargs: "main",
                policies_to_train=["main"],
            )
        )
        
        # –°–æ–∑–¥–∞–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º
        algo = config.build()
        
        # –î–µ–ª–∞–µ–º –æ–¥–∏–Ω —à–∞–≥ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        print("üèãÔ∏è Training one step to initialize...")
        result = algo.train()
        print(f"‚úÖ Training step completed, episode_reward_mean: {result.get('env_runners', {}).get('episode_reward_mean', 'N/A')}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º ONNX —ç–∫—Å–ø–æ—Ä—Ç
        print("üîß Testing ONNX export...")
        export_dir = "./test_onnx_export"
        os.makedirs(export_dir, exist_ok=True)
        
        successful_exports = export_onnx_with_meta(
            algorithm=algo,
            iteration=1,
            export_dir=export_dir,
            policies_to_export=["main"]
        )
        
        if successful_exports:
            print(f"‚úÖ ONNX export test PASSED! Exported {len(successful_exports)} policies")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
            from onnx_callbacks import run_inference_test
            
            for export in successful_exports:
                onnx_path = export["onnx_path"]
                print(f"üî¨ Testing inference for {onnx_path}...")
                
                try:
                    run_inference_test(onnx_path, batch_size=2, verbose=False)
                    print(f"‚úÖ Inference test PASSED for {export['policy_id']}")
                except Exception as e:
                    print(f"‚ùå Inference test FAILED for {export['policy_id']}: {e}")
        else:
            print("‚ùå ONNX export test FAILED - no policies exported")
            
        # –û—á–∏—Å—Ç–∫–∞
        algo.stop()
        
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        ray.shutdown()
    
    print("üèÅ ONNX export test completed")


if __name__ == "__main__":
    test_onnx_export_standalone()